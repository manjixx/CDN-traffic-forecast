import os
import sys
import json
from typing import Any, Dict, Tuple
import keras_tuner as kt
import tensorflow as tf
from keras import callbacks
from keras.optimizers import Optimizer

# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
current_path = os.path.abspath(__file__)
# è·å–é¡¹ç›®æ ¹ç›®å½•
root_path = os.path.dirname(os.path.dirname(current_path))
# å°†æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ä¸­
sys.path.append(root_path)
from configs.hyper_params import SearchSpace, ModelConfig
from configs.data_config import DataConfig


class HyperparameterTuner:
    def __init__(self, search_space: SearchSpace, model):
        self.search_space = search_space
        self.base_model = model  # å­˜å‚¨åŸºç¡€æ¨¡å‹æ¶æ„
        self.tuner: kt.Tuner = None
        self.best_hps: kt.HyperParameters = None
        self.best_model: tf.keras.Model = None

    def _build_hypermodel(self, hp: kt.HyperParameters) -> tf.keras.Model:

        """ä» SearchSpace é‡‡æ ·ç”Ÿæˆ ModelConfig"""
        # Transformer å‚æ•°é‡‡æ ·
        d_model = hp.Choice('d_model', self.search_space.d_model_range)
        nhead = hp.Choice('nhead', self.search_space.n_head_option)
        num_transformer_layers = hp.Choice('trans_layers', self.search_space.num_transformer_layers_range)

        # LSTM å‚æ•°é‡‡æ ·
        num_lstm_layers = hp.Int('num_lstm_layers', *self.search_space.lstm_layers_range)
        hidden_dims = [
            hp.Int(f'hidden_{i}_dims',
                   min_value=self.search_space.hidden_dims_range[0],
                   max_value=self.search_space.hidden_dims_range[1],
                   step=self.search_space.hidden_dims_range[2])
            for i in range(num_lstm_layers)
        ]

        # å…¬å…±å‚æ•°é‡‡æ ·
        runtime_params = {
            'dropout_rate': hp.Float('dropout', *self.search_space.dropout_range, step=0.1),
            'attention_units': hp.Int('attn_units', *self.search_space.attention_units_range),
            'learning_rate': hp.Float('lr', *self.search_space.learning_rate_range, sampling='log'),
            # 'batch_size': hp.Choice('batch_size', self.search_space.batch_size),
            'optimizer': hp.Choice('optimizer', self.search_space.optimizer_options)
        }

        # æ„å»ºå®Œæ•´é…ç½®
        config = ModelConfig(
            d_model=d_model,
            n_head=nhead,
            num_transformer_layers=num_transformer_layers,
            num_lstm_layers=num_lstm_layers,
            hidden_dims=tuple(hidden_dims),
            **runtime_params
        )

        # å®ä¾‹åŒ–å¹¶ç¼–è¯‘æ¨¡å‹
        model = self.base_model(config)
        model.compile(loss='mse', metrics=['mae'])
        return model

    def _sample_hyperparameters(self, hp: kt.HyperParameters) -> Dict[str, Any]:
        """ä»æœç´¢ç©ºé—´é‡‡æ ·å‚æ•°"""
        params = {}

        # éå†æœç´¢ç©ºé—´å®šä¹‰
        for param, config in self.search_space.param_configs.items():
            param_type = config["type"]

            try:
                if param_type == "int":
                    params[param] = hp.Int(
                        param,
                        min_value=config["min"],
                        max_value=config["max"],
                        step=config.get("step", 1)
                    )
                elif param_type == "float":
                    params[param] = hp.Float(
                        param,
                        min_value=config["min"],
                        max_value=config["max"],
                        step=config.get("step", None),
                        sampling=config.get("sampling", "linear")
                    )
                elif param_type == "category":
                    params[param] = hp.Choice(
                        param,
                        values=config["values"]
                    )
                elif param_type == "bool":
                    params[param] = hp.Boolean(
                        param,
                        default=config.get("default", False)
                    )
                else:
                    raise ValueError(f"Unsupported parameter type: {param_type}")
            except KeyError as e:
                raise ValueError(f"Missing required config for {param}: {e}")

        # å¤„ç†ä¾èµ–å‚æ•°
        if "lstm_layers" in params:
            # ä» HIDDEN_DIM ä¸­é€‰æ‹©å¯¹åº”çš„éšè—å±‚ç»´åº¦
            hidden_dims_options = self.search_space.HIDDEN_DIM
            lstm_layers = params["lstm_layers"]

            # ç¡®ä¿é€‰æ‹©çš„å±‚æ•°ä¸è¶…è¿‡ HIDDEN_DIM çš„é€‰é¡¹æ•°é‡
            if lstm_layers <= len(hidden_dims_options):
                params["hidden_dims"] = hidden_dims_options[lstm_layers - 1]  # é€‰æ‹©å¯¹åº”çš„éšè—å±‚ç»´åº¦
            else:
                raise ValueError(f"lstm_layers must be less than or equal to {len(hidden_dims_options)}")

        return params

    def _build_optimizer(self, hp: kt.HyperParameters) -> Optimizer:
        """æ„å»ºä¼˜åŒ–å™¨"""
        opt_name = hp.Choice(
            "OPTIMIZER",
            self.search_space.param_configs["OPTIMIZER"]["values"]
        )
        lr = hp.Float(
            "learning_rate",
            min_value=self.search_space.param_configs["learning_rate"]["min"],
            max_value=self.search_space.param_configs["learning_rate"]["max"],
            sampling="log"
        )

        optimizers = {
            "adam": tf.keras.optimizers.Adam,
            "rmsprop": tf.keras.optimizers.RMSprop,
            "sgd": tf.keras.optimizers.SGD
        }

        if opt_name not in optimizers:
            raise ValueError(f"Unsupported optimizer: {opt_name}")

        # å¤„ç†ä¼˜åŒ–å™¨ç‰¹æœ‰å‚æ•°
        kwargs = {}
        if opt_name == "sgd":
            kwargs["momentum"] = hp.Float(
                "sgd_momentum",
                min_value=0.0,
                max_value=0.9,
                step=0.1
            )

        return optimizers[opt_name](learning_rate=lr, **kwargs)

    def tune(
            self,
            train_data: tf.data.Dataset,
            val_data: tf.data.Dataset,
            algorithm: str = "bayesian",
            max_trials: int = 50,
            executions_per_trial: int = 2,
            objective: str = "val_loss",
            project_name: str = "hptune",
            **kwargs
    ) -> kt.HyperParameters:
        """
        æ‰§è¡Œè¶…å‚æ•°æœç´¢

        :param train_data: è®­ç»ƒæ•°æ®é›† (features, labels)
        :param val_data: éªŒè¯æ•°æ®é›† (features, labels)
        :param algorithm: è°ƒä¼˜ç®—æ³• ['bayesian', 'random', 'hyperband']
        :param max_trials: æœ€å¤§è¯•éªŒæ¬¡æ•°
        :param executions_per_trial: æ¯ä¸ªè¶…å‚æ•°ç»„åˆçš„éªŒè¯æ¬¡æ•°
        :param objective: ä¼˜åŒ–ç›®æ ‡
        :param project_name: é¡¹ç›®åç§°
        """
        # åˆå§‹åŒ–è°ƒä¼˜å™¨
        tuner_map = {
            "bayesian": kt.BayesianOptimization,
            "random": kt.RandomSearch,
            "hyperband": kt.Hyperband
        }

        self.tuner = tuner_map[algorithm.lower()](
            hypermodel=self._build_hypermodel,
            objective=kt.Objective(objective, direction="min"),
            max_trials=max_trials,
            executions_per_trial=executions_per_trial,
            directory=self.search_space.output_dir,
            project_name=project_name,
            overwrite=True
        )

        # æ·»åŠ æ•°æ®æ ¼å¼éªŒè¯
        self._validate_data_shape(train_data)
        self._validate_data_shape(val_data)

        # é…ç½®å›è°ƒå‡½æ•°
        callbacks_list = [
            callbacks.EarlyStopping(
                monitor=objective,
                patience=self.search_space.early_stopping_patience,
                restore_best_weights=True
            ),

            callbacks.CSVLogger(f"{project_name}_log.csv"),
            callbacks.TensorBoard(log_dir=f"{self.search_space.log_dir}/{project_name}")
        ]

        # æ‰§è¡Œæœç´¢
        self.tuner.search(
            x=train_data,  # ç›´æ¥ä¼ å…¥ç”Ÿæˆå™¨/Dataset
            validation_data=val_data,
            callbacks=callbacks_list,
            **kwargs
        )

        # è·å–æœ€ä¼˜å‚æ•°
        self.best_hps = self.tuner.get_best_hyperparameters(num_trials=1)[0]
        print(f"ğŸ† Optimal hyperparameters: {json.dumps(self.best_hps.values, indent=2)}")
        return self.best_hps

    def train_best_model(
            self,
            train_data: Tuple[tf.data.Dataset, tf.data.Dataset],
            val_data: Tuple[tf.data.Dataset, tf.data.Dataset],
            epochs: int = 200
    ) -> tf.keras.callbacks.History:
        """ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
        if not self.best_hps:
            raise ValueError("è¯·å…ˆæ‰§è¡Œtuneæ–¹æ³•è¿›è¡Œå‚æ•°æœç´¢")

        # æ„å»ºæœ€ç»ˆæ¨¡å‹
        self.best_model = self.tuner.hypermodel.build(self.best_hps)

        # é…ç½®ç”Ÿäº§ç¯å¢ƒå›è°ƒ
        checkpoint = callbacks.ModelCheckpoint(
            f"{self.search_space.model_dir}/best_model.h5",
            monitor="val_loss",
            save_best_only=True,
            save_weights_only=False
        )

        # æ‰§è¡Œè®­ç»ƒ
        history = self.best_model.fit(
            x=train_data[0],
            y=train_data[1],
            validation_data=val_data,
            epochs=epochs,
            callbacks=[
                checkpoint,
                callbacks.CSVLogger(f"{self.search_space.log_dir}/final_training.csv"),
                callbacks.TensorBoard(f"{self.search_space.log_dir}/final")
            ],
            verbose=1
        )
        return history

    def save_assets(self, version: str = "v1") -> None:
        """ä¿å­˜å…¨å¥—ç”Ÿäº§èµ„äº§"""
        if not self.best_model:
            raise ValueError("æœªæ‰¾åˆ°è®­ç»ƒå®Œæˆçš„æ¨¡å‹")

        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = f"{self.search_space.model_dir}/{version}"
        os.makedirs(save_dir, exist_ok=True)

        # ä¿å­˜æ¨¡å‹
        self.best_model.save(f"{save_dir}/model.h5")

        # ä¿å­˜å‚æ•°é…ç½®
        with open(f"{save_dir}/config.json", "w") as f:
            json.dump({
                "hyperparameters": self.best_hps.values,
                "fixed_parameters": self.search_space.fixed_params
            }, f, indent=2)

        print(f"âœ… ç”Ÿäº§èµ„äº§ä¿å­˜åˆ°ï¼š{save_dir}")

    def _validate_data_shape(self, data):
        """éªŒè¯æ•°æ®æ ¼å¼"""
        sample_data = next(iter(data))
        if not isinstance(sample_data, (tuple, list)) or len(sample_data) != 2:
            raise ValueError("æ•°æ®ç”Ÿæˆå™¨å¿…é¡»è¿”å›(features, labels)å…ƒç»„")
