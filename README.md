

# 巨人的肩膀
[prophet](https://blog.csdn.net/SeafyLiang/article/details/121780934)
[lstm调参经验](https://www.cnblogs.com/kamekin/p/10163743.html)
[LSTM 结构讲解](https://blog.csdn.net/u010754290/article/details/47167979)
[LSTM模型与前向反向传播算法](https://blog.csdn.net/qq_42722197/article/details/123267676)

LSTM 网络误差反向传播的主要步骤：
- 计算损失函数对输出的梯度： 首先，通过损失函数计算输出与目标值之间的误差，然后反向传播该误差，计算输出层的梯度。 
- 反向传播误差至隐藏层： 将输出层的梯度反向传播至隐藏层。在 LSTM 中，需要考虑隐藏状态、记忆单元以及各个门的梯度。 
- 计算门的梯度： 针对每个门（遗忘门、输入门、输出门），分别计算其权重和偏置的梯度。需要注意的是，门控单元的梯度计算相对复杂，需要考虑门控单元的输出以及记忆单元的状态。 
- 更新参数： 根据计算得到的梯度，使用梯度下降法或其变种（如 Adam、RMSProp 等）更新网络参数。


# LSTM的前向传播与反向传播

LSTM（长短期记忆网络，Long Short-Term Memory）是一种特殊的递归神经网络（RNN），用于处理和预测序列数据。在时间序列预测、自然语言处理等诸多应用场景中表现优异，能够学习并记住长期依赖关系，有效解决传统RNN中的梯度消失和梯度爆炸问题。以下详细阐述LSTM的前向传播和反向传播：

## 一、LSTM的前向传播（Forward Propagation）
### （一）目标
依据输入数据算出LSTM模型的输出，并让信息在LSTM的各个门（输入门、遗忘门、输出门）间传递，此为训练和预测过程中最基础的计算环节。

### （二）步骤
1. **输入**：LSTM的输入一般是一个时间步的输入 ($x_t$) 、前一时间步的隐藏状态 ($h_{t - 1}$) 以及记忆单元的状态 ($c_{t - 1}$)。
2. **门控机制**：LSTM包含三个门，每个门负责处理不同信息：
    - **遗忘门（Forget Gate）**：确定当前单元的记忆需丢弃多少，通过当前输入 ($x_t$) 和前一时刻的隐藏状态 ($h_{t - 1}$) 进行计算。
    - **输入门（Input Gate）**：判断当前输入信息中有多少要存储在单元状态中。
    - **输出门（Output Gate）**：决定下一个隐藏状态 ($h_t$) 的值。
3. **状态更新**：
    - **候选记忆单元（Cell Candidate）**：依据输入门控制的当前输入信息计算得出，随后加入到记忆单元中。
    - **记忆单元更新（Cell State Update）**：借助遗忘门控制需丢弃多少历史记忆，接着融入输入门传入的新信息，以此更新单元状态。
4. **计算输出**：输出门决定当前时刻的隐藏状态 ($h_t$)，即模型对当前时刻的预测输出。

### （三）前向传播目标
1. 算出当前时间步的隐藏状态 ($h_t$) 和单元状态 ($c_t$)，它们将作为下一个时间步的输入。
2. 通过门控机制和状态更新捕捉数据的时序特性。

## 二、LSTM的反向传播（Backward Propagation）
### （一）目标
通过计算损失函数的梯度来更新模型中的参数（如权重），这是训练LSTM的关键步骤，能促使模型逐步优化其参数，提升预测精度。

### （二）步骤
1. **计算损失**：反向传播首先计算模型输出与目标值之间的误差，即损失函数 ($\mathcal{L}$)。
2. **计算梯度**：接着，针对网络参数计算损失函数的梯度。具体到LSTM模型，需计算每一层的梯度：
    - **输出门的梯度**：计算输出门对损失的影响，这涉及到梯度在当前隐藏状态上的传播。
    - **记忆单元状态的梯度**：反向传播不仅要经过输出门，还需通过各个门的梯度反向传播至记忆单元，从而更新记忆状态。
    - **输入门和遗忘门的梯度**：计算这两个门的梯度，并更新相应的权重和偏置。
3. **梯度裁剪**：鉴于LSTM可能遭遇梯度爆炸问题，通常会对计算出的梯度进行裁剪，以防梯度过大引发不稳定。
4. **参数更新**：最后，利用梯度下降（或其他优化算法）来更新LSTM的参数，优化目标是使损失函数最小化。

### （三）反向传播目标
1. 计算各个门的梯度：通过链式法则反向传递梯度，从输出到输入、从隐藏状态到记忆单元状态。
2. 更新权重：依据计算出的梯度来更新权重和偏置，进而优化LSTM网络的性能。

## 三、总结
- **前向传播**：目标是依据输入数据，通过LSTM的各个门算出当前时刻的输出（隐藏状态）以及更新后的记忆单元状态，主要用于推理和计算模型的输出。
- **反向传播**：目标是计算损失函数的梯度，并凭借这些梯度更新LSTM模型中的参数，在训练过程中逐步减小误差，提升模型的预测精度。

两者协同配合，使LSTM能够在时序数据中高效捕捉长期依赖关系，并在训练过程中逐步优化，最终提高预测准确性。 
