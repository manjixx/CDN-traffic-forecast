为了在上述代码中集成 超参数全局搜索、统一管理窗口大小 和 输入输出步长，我们可以按照以下步骤进行改进和优化：

1. 添加超参数搜索

为了执行超参数搜索，我们可以使用**网格搜索（Grid Search）或者随机搜索（Random Search）**来优化模型的超参数。我们可以使用Keras Tuner库来实现自动化超参数搜索。

2. 统一管理窗口大小、输入输出步长

我们可以创建一个配置类或字典来统一管理输入输出的窗口大小和步长，以便灵活调整和复用。

3. 完整代码实现

下面是改进后的完整代码，集成了超参数搜索和统一管理窗口大小、输入输出步长的方案。

安装依赖

如果还没有安装 keras-tuner，可以运行以下命令进行安装：

pip install keras-tuner

改进后的代码

import tensorflow as tf
from tensorflow.keras import layers, models
import keras_tuner as kt
import numpy as np
import pandas as pd

# 配置统一管理（窗口大小、输入输出步长）
class Config:
    def __init__(self):
        self.input_seq_len = 30  # 输入序列长度
        self.output_seq_len = 5  # 输出序列长度
        self.batch_size = 32  # 批次大小
        self.epochs = 50  # 训练轮数
        self.dropout = 0.1  # Dropout率
        self.d_model = 512  # 模型维度
        self.nhead = 8  # 多头注意力头数
        self.num_layers = 6  # Transformer层数
        
config = Config()

# Autoformer模型定义
class Autoformer(tf.keras.Model):
    def __init__(self, input_seq_len, output_seq_len, d_model=512, nhead=8, num_layers=6, dropout=0.1):
        super(Autoformer, self).__init__()
        
        self.input_seq_len = input_seq_len
        self.output_seq_len = output_seq_len
        self.d_model = d_model
        self.nhead = nhead
        self.num_layers = num_layers
        self.dropout = dropout
        
        # 1. Seasonality Decomposition (Trend and Seasonal)
        self.seasonal_decomp = layers.Conv1D(filters=d_model, kernel_size=25, padding="same", activation='relu')
        
        # 2. Encoder (Trend Modeling using Transformer)
        self.encoder = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)
        self.encoder_dropout = layers.Dropout(dropout)
        
        # 3. Decoder (Seasonal Modeling using Transformer)
        self.decoder = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)
        self.decoder_dropout = layers.Dropout(dropout)

        # 4. Fully connected output layer
        self.output_layer = layers.Dense(output_seq_len)

    def call(self, inputs):
        # Step 1: Seasonal decomposition
        seasonal_part = self.seasonal_decomp(inputs)
        trend_part = inputs - seasonal_part
        
        # Step 2: Trend modeling with attention (Encoder)
        trend_output = self.encoder(trend_part, trend_part)
        trend_output = self.encoder_dropout(trend_output)
        
        # Step 3: Seasonal modeling with attention (Decoder)
        seasonal_output = self.decoder(seasonal_part, seasonal_part)
        seasonal_output = self.decoder_dropout(seasonal_output)
        
        # Combine trend and seasonal outputs
        combined_output = trend_output + seasonal_output
        
        # Step 4: Fully connected output
        output = self.output_layer(combined_output)
        
        return output

# 模型创建函数
def create_autoformer_model(hp):
    # 在超参数搜索中使用不同的超参数值
    model = Autoformer(
        input_seq_len=config.input_seq_len,
        output_seq_len=config.output_seq_len,
        d_model=hp.Int('d_model', min_value=256, max_value=1024, step=128),
        nhead=hp.Int('nhead', min_value=4, max_value=16, step=4),
        num_layers=hp.Int('num_layers', min_value=3, max_value=12, step=3),
        dropout=hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)
    )
    
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# 超参数调优
def run_hyperparameter_tuning(X_train, y_train):
    tuner = kt.Hyperband(
        create_autoformer_model,
        objective='val_loss',
        max_epochs=config.epochs,
        hyperband_iterations=2,
        directory='my_dir',
        project_name='autoformer_tuning'
    )
    
    tuner.search(X_train, y_train, epochs=config.epochs, batch_size=config.batch_size, validation_split=0.2)
    
    # 输出最佳超参数
    best_hyperparameters = tuner.get_best_hyperparameters()[0]
    print(f'Best Hyperparameters: {best_hyperparameters}')
    
    # 使用最佳超参数创建最终模型
    best_model = tuner.hypermodel.build(best_hyperparameters)
    return best_model

# 数据准备
def prepare_sequences(data, input_seq_len=30, output_seq_len=5):
    X = []
    y = []
    for i in range(len(data) - input_seq_len - output_seq_len):
        X.append(data[i:i+input_seq_len])
        y.append(data[i+input_seq_len:i+input_seq_len+output_seq_len])
    return np.array(X), np.array(y)

# 数据加载示例
data = np.sin(np.linspace(0, 1000, 10000))  # 举例模拟数据
df = pd.DataFrame(data, columns=["value"])

# 准备数据集
X, y = prepare_sequences(df["value"].values)

# Train/test split
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# 训练与超参数调优
best_model = run_hyperparameter_tuning(X_train, y_train)

# 训练最佳模型
best_model.fit(X_train, y_train, epochs=config.epochs, batch_size=config.batch_size)

# 模型评估
best_model.evaluate(X_test, y_test)

4. 代码分析
	1.	超参数调优（Hyperparameter Tuning）：
	•	使用 Keras Tuner 实现超参数调优。
	•	我们定义了 d_model、nhead、num_layers 和 dropout 四个可调超参数，Keras Tuner 会自动搜索最优超参数组合。
	•	Hyperband 是一种非常高效的超参数搜索策略，适用于大规模的超参数调优。
	2.	统一管理窗口大小与步长：
	•	使用 Config 类统一管理窗口大小、输入输出步长等超参数。这使得模型的参数配置更具可维护性和可扩展性。
	3.	数据准备：
	•	使用滑动窗口方法将时间序列数据拆分成输入和输出序列对。
	•	每个输入序列包含过去30天的带宽数据，输出序列是接下来5天的带宽数据。
	4.	训练与评估：
	•	fit() 方法用于训练模型，evaluate() 用于评估模型在测试集上的表现。
	•	超参数调优完成后，选取最佳的超参数并进行训练。

5. 训练与调优建议
	•	训练时间：由于模型会有多个超参数配置，训练时间可能较长。可以考虑使用GPU进行加速。
	•	数据准备：根据你的实际应用，可以调整输入序列的长度（如30天），以及输出序列的长度（如5天）。
	•	超参数调优：可以通过调节 Keras Tuner 的 max_epochs 和 hyperband_iterations 来控制搜索的规模和训练时间。

通过这种方法，你可以自动化搜索超参数，同时确保在代码中统一管理窗口大小和步长，从而实现灵活且高效的模型训练。
